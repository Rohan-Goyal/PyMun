#+LATEX_HEADER: \usepackage[margin=0.8in]{geometry}
#+LATEX_HEADER: \raggedright
#+OPTIONS: toc:nil
* Planning for the MUN product
Today:
- Comprehensive spec
- Ms. Rizwana
- Draft crit A.

Reqs
Filesearch
Data structure manip
Polymorph
# Recursion
OOP stuff
= Libraries
Multi-dimensional arrays
= Complex data structs

** Ideas
[Digitile is a similar tool]
*** Web
- GDocs, etc. plugin


*** Python
- Gdocs tool. In essence, it has full access to a folder. Within that folder, there are a bunch of MUN files

** Data struct thinking
-
-
** Kinds of documents
-
* Re-imagining the document classifier
** Classifier
*** Heuristics
- DONE-ish List indentation
- DONE Metadata presence? That's actually an interesting one, which I haven't considered before. If it has country, it's a position paper.
- Para structure
- DONE Links
- DONE List density (how much of the document is list)
- More heuristics for identifying position papers. Draw the flowchart to make sure the ifs and fors are nested properly.
**** Position paper
- Short
- Not many paragraphs
- Metadata DONE
- No links
**** Plan
The issue is reliably distinguishing between position papers and notes. So we should restructure the classifier function. There are two ways we can go: Simple and sophisticated
***** Simple DONE
- First, run a bunch of tests (list-based) to determine if it is or isn't a resolution. If it is, exit and return. We have a decent battery of tests so be a bit certain here
- If not, run tests such as links, para structure, metadata, to check if it's position or notes. This is iffy, so don't be afraid to call it 'unclassified' if necessary.
***** Complex
- Break the document into a tree structure, and do all this stuff to the tree. This is just a nice-to-have, so don't implement it until everything else is done.

**** Para structure testing
What are our options here? If it's >2 pages (How to check this) return notes
If it's less than 2 pages, we can't consider paragraph length, but we can consider number of paragraphs.
Maybe section count helps, but doubt it. So just use wordcounts, I guess. NOTE: Roughly 600 words/page. So that means <(900-1000) is our check.

So now how do we go about this?
We have the number of paras, and each of their wordcounts.
From here we can do simple stats:  max,min, mean, etc.
How can we use these? I'm not optimistic that we can, honestly.

** Metadata extractor
Question: Should we extract metadata after classification, or before? After might be better, since then we'll know what meta we can expect to look for. But we'll also need to take into account blackbox leakages from the classifier.
So we should do it after, but include some checks to make sure we aren't looking for something that isn't there.
For now, we have an ok hacky solution.
Getting note metadata reliably: No idea. Basically, we can give them null metadata, or we can search for existing metadata(existing agenda etc which we've found from parsing papers and resolutions). This last one would be messy, and require us to create a messy text file. So every time we parse metadata, we also write it to a text file. Not in the spec, so don't do it for now. So this isn't going to be implemented for now"""

** Treemaker

** DONE Autoformat
Don't put too much work into this - make it perpetually beta, and warn people not to use it (Add that warning as placeholder (in red, naturally), into the main UI)
* Custom rules
** DONE UI
Define a 'component' of sorts (a horizontal form) that represents a customRule. When webform is initialised, it should read the custom rules and create components for each of them. Wrap all the components in a div or whatever. After the div, have a + button that adds a form element to the div using JS. This will be messy, and require duplication between JS and Py.
*** DONE
- Define component as JINJA template and/or JS var
- Add button that calls the 'newRule' method
- Define newRule in js.
- Define autoload func in python (to automatically read the rules)
- Define the py methods for writing these from the existing file: Add it to the conf_dict and prepopulate methods
- That's basically it.
 Oh god, we'll need to work with javascript or some kind of dynamic HTML.
** DONE Implementation
Basically, they give us three values: (Contains/name),(string),(classification).
We should call this from the updateMetadata func. We first check the namerules, then the containRules, and if none of those work we use magicParse. So it accepts a result obtained by downloadHelper, which has the title, localpath, etc.
So we pass the file object directly. We try it against the namerules, or
* Notes
# - Time complexity/elegance
# - Separation accomplished through templates
# - Abstraction accomplished
# - Global variables: I only require one.
# - Questions: How many flowcharts for each criterion?
* DONE Feedback <2020-12-08 Tue>
- No names in final document (Format)
- Add TOC of sorts of complex code techniques. (Format)
- Defend why I used techniques: Too many buzzwords as it stands. Focus on justification/valuing
- Make it clear.
# - Go through and remove the unnecessary words.
- Remove the jokes
- Add in-text citations where apt, for things like trees and APIs.
# - Consider briefly annotating some entries with things like <library>, <documentation>, <theoretical reference>, etc. (Think about how to do this)
# - Consider doing that via formatting/colours
# - Add a blanket disclaimer to sources
* DONE Todo items
- Ask Ms. Almas about personal information in the video
# - Ask her if I should include results from unit tests, etc. in criterion D/E
- Ms. Rizwana: Feedback via text/email
- Email thread appendix with Ms. Rizwana for crit A, outlining solutions and problems - Crit A: At least 3 pieces of evidence of consultation with client
- Good to keep a few failed criteria.
# - For test results: I should have them in criterion E as screenshots, but including some in the video is also good form
# - Flesh out crit D,E record of tasks
- GANTT
# - Structure diagram: Firstly a user story/workflow, and then possibly a listing of functions
- Consider justifying design and data structures briefly
# - Flowcharts: About 5-6. So add more
# - It seems UML should be in crit B
# - Flowcharts: B should have overview, and most of the charts
# - C should have charts for specific fiddly bits.
- Consider adding focus on client/objective in crit C
- Add crit C appendix, acknowledging libraries used
-  In Crit D, include evidence of error handling and the like.
- In crit D video, add extensibility discussion: Discussion of layers for heuristics, and how multiple heuristics can be composed (such as the custom/builtin heuristics) to create sophisticated filters/analyses.
* DONE <2021-02-13 Sat>
- Crit D: Add extensibility/layer discussion, and heuristic composition
- Appendices
# - GANTT

# - Make sure each criterion is covered by the test plan.
* Consulting
- Compose message to Ms. Rizwana about this.
- Send her brief outline of designs
- Ask her what she thinks about it

* Submission
Use forms.zip structure
# HTML cover page

Folder:
Subfolder docs (including video)
Subfolder product DONE

* TODO Feedback <2021-02-23 Tue>
- Appendices: Refine
- Remove email IDs (since those give school info) from appendix
- Appendices for evidence of consultation.
# - DONE, mostly. Split appendices into separate documents: each named appendix A-1, E-1, E-2, etc. as appropriate
# - DONE Add borders to the crit E table.
# - DONE At opening of E, note where client feedback is discussed.
- Explicitly reference the client needs, etc. when discussing improvements in crit E.
- (Recommended highly) README.org file, distributed with the code

** Appendices: 6 total
# 1. DONE Initial consultation (problem identification)
# 2. DONE (I think it's done) Detailed interview for product and rationale
3. Detailed interview for success criteria (TODO: org)
# 4. DONE Detailed interview about prototype/designs (crit B). Reach out to her directly. If I have notes, use those, otherwise conduct a formal interview
# 5. DONE Code listing (word document) of all files
6. Client feedback on success criteria (max of 3 pages). Ideally, feedback of each success criteria. Can be notes or interview transcript. Explicitly reference this appendix in crit E. (TODO: Docx)

NOTE: Make sure to reference appendices in documentation.
